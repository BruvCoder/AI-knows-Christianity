{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BruvCoder/AI-knows-Christianity/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "QKhOd8bX3JdW",
        "collapsed": true,
        "outputId": "74246c62-2e93-40ce-8f38-e24969192adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "--- Epoch 1 of 3 ---\n",
            "Batch 100/219, Training Loss: 0.0093\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-811068110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "\n",
        "# --- Setup and Data Loading ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "df = pd.read_csv('/content/christian_sentences_10000.csv')\n",
        "data = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(data['Sentences']), np.array(data['Label']), test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# --- Efficient Dataset Class ---\n",
        "class ChristianDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, max_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.X[idx]\n",
        "        label = self.y[idx]\n",
        "\n",
        "        # Tokenize each sample on the fly\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 384),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)[0][:, 0]\n",
        "        output = self.dropout(bert_output)\n",
        "        output = self.classifier(output)\n",
        "        return output.squeeze()\n",
        "\n",
        "# --- Instantiating and Training ---\n",
        "max_length = 100\n",
        "training_data = ChristianDataset(X_train, y_train, tokenizer, max_length)\n",
        "validation_data = ChristianDataset(X_val, y_val, tokenizer, max_length)\n",
        "\n",
        "training_dataLoader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "validation_dataLoader = DataLoader(validation_data, batch_size=32, shuffle=False) # No need to shuffle validation data\n",
        "\n",
        "# Instantiate the model with BERT\n",
        "model = MyModel(bert_model).to(device)\n",
        "\n",
        "# A lower learning rate is crucial for fine-tuning BERT\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "epochs = 3 # Start with fewer epochs for fine-tuning\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"--- Epoch {epoch+1} of {epochs} ---\")\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, labels) in enumerate(training_dataLoader):\n",
        "        input_ids = inputs['input_ids'].to(device)\n",
        "        attention_mask = inputs['attention_mask'].to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f\"Batch {batch_idx+1}/{len(training_dataLoader)}, Training Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in validation_dataLoader:\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "            attention_mask = inputs['attention_mask'].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            validation_loss += loss_fn(outputs, labels).item()\n",
        "\n",
        "    avg_val_loss = validation_loss / len(validation_dataLoader)\n",
        "    print(f\"Epoch {epoch+1} finished. Average Validation Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# --- Inference Function ---\n",
        "def predict(text):\n",
        "    \"\"\"\n",
        "    Predicts the label and confidence for a given text.\n",
        "    \"\"\"\n",
        "    # Tokenize the input text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=100,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        # Get the model's output probability\n",
        "        output = model(input_ids, attention_mask)\n",
        "\n",
        "    # Classify the prediction based on a 0.5 threshold\n",
        "    predicted_class = (output > 0.5).int().item()\n",
        "\n",
        "    # Determine the label based on the class\n",
        "    if predicted_class == 1:\n",
        "        label = \"True\"\n",
        "        confidence_score = output.item() * 100\n",
        "    else:\n",
        "        label = \"False\"\n",
        "        confidence_score = (1 - output.item()) * 100\n",
        "\n",
        "    return label, confidence_score\n",
        "\n",
        "# --- Example Usage ---\n",
        "test_text = \"Jesus Christ is lord and saviour\"\n",
        "label, confidence = predict(test_text)\n",
        "print(f\"Text: '{test_text}'\")\n",
        "print(f\"Predicted Label: {label}\")\n",
        "print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "test_text2 = \"God is not triune\"\n",
        "label2, confidence2 = predict(test_text2)\n",
        "print(f\"Text: '{test_text2}'\")\n",
        "print(f\"Predicted Label: {label2}\")\n",
        "print(f\"Confidence: {confidence2:.2f}%\")"
      ],
      "metadata": {
        "id": "yU7XWUNzXQlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhmz9wTc7Q4XX0HxAkcbiV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}